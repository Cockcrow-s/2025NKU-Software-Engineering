import cv2
import logging
import argparse
import warnings
import numpy as np
import time
import matplotlib.pyplot as plt
from collections import deque
import threading
from queue import Queue

import torch
import torch.nn.functional as F
from torchvision import transforms

from config import data_config
from utils.helpers import get_model, draw_bbox_gaze

import uniface

warnings.filterwarnings("ignore")
logging.basicConfig(level=logging.INFO, format='%(message)s')


class VisualRecognitionInterface:
    """
    è§†è§‰è¯†åˆ«æ¥å£ç±»ï¼Œæä¾›ç»™face_thread.pyè°ƒç”¨
    æ•´åˆäº†è§†çº¿è¿½è¸ªå’Œå¤´éƒ¨å§¿åŠ¿è¯†åˆ«åŠŸèƒ½
    """
    def __init__(self, model_name="resnet18", weight_path="weights/18.pt", dataset="mpiigaze"):
        """
        åˆå§‹åŒ–è§†è§‰è¯†åˆ«æ¥å£
        :param model_name: è§†çº¿ä¼°è®¡æ¨¡å‹åç§°
        :param weight_path: æƒé‡æ–‡ä»¶è·¯å¾„
        :param dataset: æ•°æ®é›†åç§°ï¼Œç”¨äºè·å–é…ç½®
        """
        # è®¾ç½®è®¾å¤‡
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.dataset = dataset
        
        # æ ¹æ®æ•°æ®é›†åŠ è½½é…ç½®
        if dataset in data_config:
            self.config = data_config[dataset]
            self.bins = self.config["bins"]
            self.binwidth = self.config["binwidth"]
            self.angle = self.config["angle"]
        else:
            raise ValueError(f"Unknown dataset: {dataset}")
        
        # åˆ›å»ºç´¢å¼•å¼ é‡
        self.idx_tensor = torch.arange(self.bins, device=self.device, dtype=torch.float32)
        
        # åˆå§‹åŒ–äººè„¸æ£€æµ‹å™¨
        self.face_detector = uniface.RetinaFace()
          # åŠ è½½è§†çº¿ä¼°è®¡æ¨¡å‹
        print("ğŸ” æ­£åœ¨åŠ è½½è§†çº¿ä¼°è®¡æ¨¡å‹...")
        try:
            self.gaze_detector = get_model(model_name, self.bins, inference_mode=True)
            print(f"âœ“ æ¨¡å‹æ¶æ„åŠ è½½æˆåŠŸï¼š{model_name}")
            
            print(f"ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡ï¼š{weight_path}")
            state_dict = torch.load(weight_path, map_location=self.device)
            self.gaze_detector.load_state_dict(state_dict)
            self.gaze_detector.to(self.device)
            self.gaze_detector.eval()
            
            print(f"âœ… è§†çº¿ä¼°è®¡æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            print(f"   - è®¾å¤‡ï¼š{self.device}")
            print(f"   - æ•°æ®é›†ï¼š{dataset}")
            print(f"   - è§’åº¦åˆ†binsï¼š{self.bins}")
            logging.info(f"âœ… Gaze Estimation model '{model_name}' weights loaded successfully on {self.device}.")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
            logging.error(f"âŒ Exception occurred while loading gaze model: {e}")
            raise
          # å¤´éƒ¨å§¿åŠ¿è¯†åˆ«å™¨
        self.head_gesture_recognizer = HeadGestureRecognizer()
        
        # åˆ†å¿ƒæ£€æµ‹ç›¸å…³å˜é‡
        self.safe_angle = 0.1  # è®¾ç½®ä¸ºå›ºå®šçš„å®‰å…¨è§’åº¦å€¼ï¼ˆ0.1å¼§åº¦ï¼Œçº¦5.7åº¦ï¼‰
        self.distraction_start_time = None
        self.distracted = False
        self.warning_level = 0
        self.distraction_duration = 0
        # å›è°ƒå‡½æ•°
        self.distraction_callback = None
        self.gesture_callback = None
        
        # è§†çº¿å†å²
        self.angle_history = deque(maxlen=100)
    
    def set_distraction_callback(self, callback):
        """è®¾ç½®åˆ†å¿ƒæ£€æµ‹å›è°ƒå‡½æ•°"""
        self.distraction_callback = callback
        
    def set_gesture_callback(self, callback):
        """è®¾ç½®æ‰‹åŠ¿æ£€æµ‹å›è°ƒå‡½æ•°"""
        self.gesture_callback = callback
    
    def process_frame(self, frame):
        """
        å¤„ç†å•å¸§å›¾åƒ
        :param frame: è¾“å…¥å¸§
        :return: åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        result = {
            "distracted": False, 
            "warning_level": 0,
            "gaze_angle": 0,
            "gesture": None,
            "face_detected": False,
            "face_info": None
        }
        
        if frame is None:
            return result
        
        # äººè„¸æ£€æµ‹
        bboxes, keypoints = self.face_detector.detect(frame)
        
        # å¤„ç†å¤´éƒ¨å§¿åŠ¿
        head_gesture = None
        if len(bboxes) > 0:
            # å–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            keypoints_first_face = keypoints[0]
            
            # ä¼°è®¡å¤´éƒ¨å§¿åŠ¿
            pitch, yaw, roll = self.head_gesture_recognizer.estimate_head_pose(keypoints_first_face, frame.shape)
            
            if pitch is not None and yaw is not None:
                # æ£€æµ‹å¤´éƒ¨å§¿åŠ¿
                head_gesture = self.head_gesture_recognizer.detect_gesture(pitch, yaw, roll)
                
                # å¦‚æœæœ‰æ‰‹åŠ¿ï¼Œè°ƒç”¨å›è°ƒ
                if head_gesture and self.gesture_callback:
                    self.gesture_callback(head_gesture)
                
                # æ›´æ–°ç»“æœ
                result["gesture"] = head_gesture
            
            # æ›´æ–°ç»“æœä¸­çš„äººè„¸ä¿¡æ¯
            result["face_detected"] = True
            bbox = bboxes[0]
            x_min, y_min, x_max, y_max = map(int, bbox[:4])
            h, w = frame.shape[:2]
            x_min = max(0, x_min)
            y_min = max(0, y_min)
            x_max = min(w, x_max)
            y_max = min(h, y_max)
            
            # è·³è¿‡æ— æ•ˆæ¡†
            if x_max <= x_min or y_max <= y_min:
                return result
                
            face_image = frame[y_min:y_max, x_min:x_max]
            
            # è·³è¿‡ç©ºå›¾åƒ
            if face_image.size == 0:
                return result
            
            # è§†çº¿ä¼°è®¡å¤„ç†
            try:
                image = pre_process(face_image)
                image = image.to(self.device)
                
                with torch.no_grad():
                    pitch, yaw = self.gaze_detector(image)
                    pitch_predicted, yaw_predicted = F.softmax(pitch, dim=1), F.softmax(yaw, dim=1)
                    pitch_predicted = torch.sum(pitch_predicted * self.idx_tensor, dim=1) * self.binwidth - self.angle
                    yaw_predicted = torch.sum(yaw_predicted * self.idx_tensor, dim=1) * self.binwidth - self.angle
                    pitch_predicted = np.radians(pitch_predicted.cpu().numpy()[0])
                    yaw_predicted = np.radians(yaw_predicted.cpu().numpy()[0])
                  # è®¡ç®—è§†çº¿ä¸æ­£å‰æ–¹çš„3Då¤¹è§’theta
                theta = np.arccos(np.cos(pitch_predicted) * np.cos(yaw_predicted))
                self.angle_history.append(theta)
                
                # åˆ†å¿ƒæ£€æµ‹é€»è¾‘ - é©¾é©¶åœºæ™¯
                current_time = time.time()
                if abs(theta) > self.safe_angle:
                    if self.distraction_start_time is None:
                        self.distraction_start_time = current_time  # è®°å½•åˆ†å¿ƒèµ·å§‹æ—¶é—´
                    else:
                        self.distraction_duration = current_time - self.distraction_start_time
                        
                        # æŒ‰ç…§éœ€æ±‚ï¼Œè¶…è¿‡3ç§’è®¤ä¸ºåˆ†å¿ƒ
                        if self.distraction_duration > 3:
                            self.distracted = True
                            
                            # åŠ¨æ€è°ƒæ•´è­¦å‘Šç­‰çº§ï¼š
                            # 1çº§: 3-5ç§’ è½»åº¦è­¦å‘Š
                            # 2çº§: 5-8ç§’ ä¸­åº¦è­¦å‘Š
                            # 3çº§: >8ç§’ ä¸¥é‡è­¦å‘Š
                            if self.distraction_duration > 8:
                                self.warning_level = 3  # ä¸¥é‡è­¦å‘Š
                            elif self.distraction_duration > 5:
                                self.warning_level = 2  # ä¸­åº¦è­¦å‘Š
                            else:
                                self.warning_level = 1  # è½»åº¦è­¦å‘Š
                else:
                    # è§†çº¿å›åˆ°å‰æ–¹ï¼Œä½†ä¿æŒçŸ­æš‚çš„ç¼“å†²æœŸ
                    if self.distracted and self.distraction_start_time is not None:
                        # å¦‚æœä¹‹å‰å¤„äºåˆ†å¿ƒçŠ¶æ€ï¼Œç»™äºˆ1ç§’çš„ç¼“å†²æœŸ
                        buffer_time = 1.0
                        if current_time - self.distraction_start_time < buffer_time:
                            # ä¿æŒåˆ†å¿ƒçŠ¶æ€ï¼Œä½†ä¸å¢åŠ è­¦å‘Šç­‰çº§
                            pass
                        else:
                            self.distraction_start_time = None
                            self.distracted = False
                            self.warning_level = 0
                            self.distraction_duration = 0
                    else:
                        self.distraction_start_time = None
                        self.distracted = False
                        self.warning_level = 0
                        self.distraction_duration = 0
                
                # è°ƒç”¨åˆ†å¿ƒå›è°ƒ
                if self.distraction_callback and (self.distracted or result["distracted"] != self.distracted):
                    self.distraction_callback(self.distracted, self.warning_level)
                
                # æ›´æ–°ç»“æœ
                result["distracted"] = self.distracted
                result["warning_level"] = self.warning_level
                result["gaze_angle"] = theta
                result["face_info"] = {
                    "bbox": [x_min, y_min, x_max, y_max],
                    "keypoints": keypoints_first_face,
                    "pitch": pitch_predicted,
                    "yaw": yaw_predicted,
                    "head_pitch": pitch,
                    "head_yaw": yaw,
                    "head_roll": roll
                }
                
            except Exception as e:
                logging.error(f"Error in gaze estimation: {e}")
        
        return result
        
    def render_info_on_frame(self, frame, result):
        """
        åœ¨å¸§ä¸Šæ¸²æŸ“åˆ†æä¿¡æ¯
        :param frame: è¾“å…¥å¸§
        :param result: process_frameè¿”å›çš„ç»“æœ
        :return: æ¸²æŸ“åçš„å¸§
        """
        if not result["face_detected"]:
            return frame
            
        face_info = result["face_info"]
        bbox = face_info["bbox"]
        keypoints = face_info["keypoints"]
        
        # ç»˜åˆ¶äººè„¸è¾¹ç•Œæ¡†
        x_min, y_min, x_max, y_max = bbox
        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
          # ç»˜åˆ¶å…³é”®ç‚¹
        for i, kp in enumerate(keypoints):
            x, y = int(kp[0]), int(kp[1])
            cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)
            # æ ‡æ³¨å…³é”®ç‚¹
            labels = ["L_Eye", "R_Eye", "Nose", "L_Mouth", "R_Mouth"]
            if i < len(labels):
                cv2.putText(frame, labels[i], (x+5, y-5), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 0), 1)
        
        # ç»˜åˆ¶è§†çº¿æ–¹å‘
        draw_bbox_gaze(frame, bbox, face_info["pitch"], face_info["yaw"])
        
        # ç»˜åˆ¶è§†çº¿è§’åº¦ä¿¡æ¯
        theta = result['gaze_angle']
        safe_threshold = 0.1  # å®‰å…¨è§’åº¦é˜ˆå€¼
        
        # æ ¹æ®è§†çº¿è§’åº¦è®¾ç½®é¢œè‰²
        if theta > safe_threshold:
            color = (0, 0, 255)  # çº¢è‰² - åˆ†å¿ƒ
            status = "DISTRACTED"
        else:
            color = (0, 255, 0)  # ç»¿è‰² - æ­£å¸¸
            status = "FOCUSED"
            
        # æ˜¾ç¤ºè§†çº¿çŠ¶æ€
        cv2.putText(frame, f"Gaze: {status}", 
                   (x_min, y_min-40), cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
        cv2.putText(frame, f"Theta: {theta:+.3f} rad", 
                    (x_min, y_min-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)
        
        # ç»˜åˆ¶è§†çº¿è¿½è¸ªå†å²è½¨è¿¹
        if hasattr(self, 'angle_history') and len(self.angle_history) > 1:
            history_points = list(self.angle_history)
            for i in range(1, len(history_points)):
                # å°†è§’åº¦å€¼æ˜ å°„åˆ°å±å¹•åæ ‡
                y1 = int(30 + history_points[i-1] * 200)  # æ”¾å¤§æ˜¾ç¤º
                y2 = int(30 + history_points[i] * 200)
                x1 = frame.shape[1] - len(history_points) + i - 1
                x2 = frame.shape[1] - len(history_points) + i
                
                # æ ¹æ®è§’åº¦å€¼è®¾ç½®è½¨è¿¹é¢œè‰²
                if history_points[i] > safe_threshold:
                    trail_color = (0, 0, 255)  # çº¢è‰²è½¨è¿¹
                else:
                    trail_color = (0, 255, 0)  # ç»¿è‰²è½¨è¿¹
                    
                cv2.line(frame, (x1, y1), (x2, y2), trail_color, 2)
            
            # ç»˜åˆ¶å®‰å…¨çº¿
            safe_y = int(30 + safe_threshold * 200)
            cv2.line(frame, (frame.shape[1]-100, safe_y), 
                    (frame.shape[1]-10, safe_y), (255, 255, 0), 2)
            cv2.putText(frame, "Safe Line", 
                       (frame.shape[1]-100, safe_y-10), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 1)
        
        # æ˜¾ç¤ºå¤´éƒ¨å§¿åŠ¿
        if result["gesture"] == "nod":
            cv2.putText(frame, "Nod (Confirm)", (30, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
        elif result["gesture"] == "shake":
            cv2.putText(frame, "Shake (Reject)", (30, 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)        # åˆ†å¿ƒè­¦å‘Šæ˜¾ç¤º - å¢å¼ºçš„çº¢è‰²è¾¹æ¡†è­¦å‘Š
        if result["distracted"]:
            now = time.time()
            # æ ¹æ®è­¦å‘Šç­‰çº§è°ƒæ•´é—ªçƒé¢‘ç‡
            flash_frequency = 2 + result["warning_level"]  # æ¯ç§’é—ªçƒæ¬¡æ•°
            flash = int(now * flash_frequency) % 2 == 0
            
            # åŠ¨æ€è°ƒæ•´è­¦å‘Šæ–‡æœ¬å’Œé¢œè‰²
            warning_level = result["warning_level"]
            if warning_level == 1:
                warning_text = "âš ï¸ è­¦å‘Šï¼è¯·ç›®è§†å‰æ–¹"
                border_color = (0, 100, 255)  # æ©™çº¢è‰²
            elif warning_level == 2:
                warning_text = "ğŸš¨ è­¦å‘Šï¼è¯·ç«‹å³ç›®è§†å‰æ–¹"
                border_color = (0, 50, 255)   # æ·±çº¢è‰²
            else:  # çº§åˆ«3
                warning_text = "ğŸ†˜ å±é™©ï¼è¯·ç«‹å³ç›®è§†å‰æ–¹ï¼"
                border_color = (0, 0, 255)    # çº¯çº¢è‰²
            
            if flash:
                # ç»˜åˆ¶å¤šå±‚çº¢è‰²è¾¹æ¡†è­¦å‘Š
                h, w = frame.shape[:2]
                
                # å¤–å±‚è¾¹æ¡† - æœ€ç²—
                border_thickness = 8 + warning_level * 4
                cv2.rectangle(frame, (0, 0), (w-1, h-1), border_color, border_thickness)
                
                # ä¸­å±‚è¾¹æ¡†
                cv2.rectangle(frame, (15, 15), (w-16, h-16), (0, 0, 255), 6)
                
                # å†…å±‚è¾¹æ¡†
                cv2.rectangle(frame, (25, 25), (w-26, h-26), (255, 255, 255), 2)
                
                # åœ¨å››ä¸ªè§’è½æ·»åŠ è­¦å‘Šæ ‡å¿—
                corner_size = 30
                # å·¦ä¸Šè§’
                cv2.rectangle(frame, (0, 0), (corner_size, corner_size), (0, 0, 255), -1)
                cv2.putText(frame, "!", (8, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                # å³ä¸Šè§’
                cv2.rectangle(frame, (w-corner_size, 0), (w, corner_size), (0, 0, 255), -1)
                cv2.putText(frame, "!", (w-22, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                # å·¦ä¸‹è§’
                cv2.rectangle(frame, (0, h-corner_size), (corner_size, h), (0, 0, 255), -1)
                cv2.putText(frame, "!", (8, h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
                
                # å³ä¸‹è§’
                cv2.rectangle(frame, (w-corner_size, h-corner_size), (w, h), (0, 0, 255), -1)
                cv2.putText(frame, "!", (w-22, h-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
            
            # æ˜¾ç¤ºè­¦å‘Šæ–‡æœ¬ - å±…ä¸­æ˜¾ç¤º
            text_size = cv2.getTextSize(warning_text, cv2.FONT_HERSHEY_SIMPLEX, 1.2, 3)[0]
            text_x = (frame.shape[1] - text_size[0]) // 2
            text_y = 100
            
            # æ–‡æœ¬èƒŒæ™¯
            cv2.rectangle(frame, (text_x-10, text_y-35), 
                         (text_x+text_size[0]+10, text_y+10), (0, 0, 0), -1)
            cv2.rectangle(frame, (text_x-10, text_y-35), 
                         (text_x+text_size[0]+10, text_y+10), (0, 0, 255), 3)
            
            # è­¦å‘Šæ–‡æœ¬
            cv2.putText(frame, warning_text, (text_x, text_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 3)
            
            # æ˜¾ç¤ºåˆ†å¿ƒæŒç»­æ—¶é—´
            if hasattr(self, 'distraction_duration'):
                duration_text = f"åˆ†å¿ƒæ—¶é•¿: {self.distraction_duration:.1f}s"
                cv2.putText(frame, duration_text, (text_x, text_y+30), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)
                
                # æ ¹æ®è­¦å‘Šçº§åˆ«è°ƒæ•´è¾¹æ¡†ç²—ç»†
                border_thickness = 4 + warning_level * 2
                cv2.rectangle(frame, (20, 20), 
                             (frame.shape[1]-20, frame.shape[0]-20), 
                             (0,0,255), border_thickness)  # çº¢è‰²é—ªçƒè¾¹æ¡†
                
                # é«˜çº§åˆ«è­¦å‘Šé¢å¤–è§†è§‰æ•ˆæœ
                if warning_level >= 3:
                    # æ·»åŠ é¢å¤–çš„è­¦å‘ŠæŒ‡ç¤º
                    cv2.rectangle(frame, (40, 40), 
                                 (frame.shape[1]-40, frame.shape[0]-40), 
                                 (0,255,255), border_thickness-2)  # é»„è‰²å†…è¾¹æ¡†
        
        return frame


class HeadGestureRecognizer:
    def __init__(self, frame_queue=None):
        # ä½¿ç”¨RetinaFaceæ›¿ä»£MediaPipe
        self.face_detector = uniface.RetinaFace()
        self.last_pitch = None  # ä¸Šä¸€å¸§pitch
        self.last_yaw = None    # ä¸Šä¸€å¸§yaw
        self.last_roll = None   # ä¸Šä¸€å¸§roll
        self.nod_counter = 0    # ç‚¹å¤´è®¡æ•°
        self.shake_counter = 0  # æ‘‡å¤´è®¡æ•°
        self.frame_queue = frame_queue  # ä¸»è¿›ç¨‹ä¼ å…¥çš„å¸§é˜Ÿåˆ—
        self.nod_threshold = 0.05  # ç‚¹å¤´åˆ¤æ®é˜ˆå€¼
        self.shake_threshold = 0.05  # æ‘‡å¤´åˆ¤æ®é˜ˆå€¼
        self.roll_limit = 0.3  # ä¾§å€¾è¿‡å¤§æ—¶ä¸åˆ¤å®š
        self.nod_frames = 3    # ç‚¹å¤´éœ€è¿ç»­å¸§
        self.shake_frames = 3  # æ‘‡å¤´éœ€è¿ç»­å¸§
        # ä¸Šä¸€å¸§å…³é”®ç‚¹ä½ç½®
        self.prev_keypoints = None

    def estimate_head_pose(self, keypoints, image_shape):
        """
        åŸºäºäººè„¸å…³é”®ç‚¹ç®€å•ä¼°è®¡å¤´éƒ¨å§¿æ€ï¼ˆpitch/yaw/rollï¼‰
        :param keypoints: RetinaFaceæ£€æµ‹çš„äººè„¸5ä¸ªå…³é”®ç‚¹
        :param image_shape: å›¾åƒå°ºå¯¸
        :return: pitch, yaw, roll
        """
        # æ³¨æ„ï¼šæ­¤ä¸ºç®€åŒ–å®ç°ï¼Œå®é™…æ•ˆæœå¯èƒ½ä¸å¦‚mediapipeçš„ç²¾ç¡®
        image_h, image_w = image_shape[:2]
        
        # æå–çœ¼ç›ã€é¼»å°–å’Œå˜´å·´å…³é”®ç‚¹
        left_eye = keypoints[0]
        right_eye = keypoints[1]
        nose = keypoints[2]
        left_mouth = keypoints[3]
        right_mouth = keypoints[4]
        
        # è®¡ç®—çœ¼ç›ä¸­å¿ƒç‚¹å’Œå˜´å·´ä¸­å¿ƒç‚¹
        eye_center = [(left_eye[0] + right_eye[0])/2, (left_eye[1] + right_eye[1])/2]
        mouth_center = [(left_mouth[0] + right_mouth[0])/2, (left_mouth[1] + right_mouth[1])/2]
        
        # è®¡ç®—çœ¼ç›æ°´å¹³çº¿æ–œç‡ (yaw)
        eye_dx = right_eye[0] - left_eye[0]
        eye_dy = right_eye[1] - left_eye[1]
        if eye_dx != 0:
            yaw = np.arctan(eye_dy / eye_dx)  # å·¦å³è½¬åŠ¨
        else:
            yaw = 0
            
        # è®¡ç®—é¼»å­åˆ°çœ¼ç›ä¸­å¿ƒå’Œå˜´å·´ä¸­å¿ƒçš„å‚ç›´å…³ç³» (pitch)
        vertical_line = [eye_center[1], mouth_center[1]]
        nose_offset = nose[1] - (eye_center[1] + mouth_center[1])/2
        pitch = nose_offset / (vertical_line[1] - vertical_line[0]) * 0.5  # ä¸Šä¸‹ç‚¹å¤´
        
        # è®¡ç®—çœ¼ç›çº¿ç›¸å¯¹æ°´å¹³çº¿çš„æ—‹è½¬ (roll)
        eye_angle = np.arctan2(eye_dy, eye_dx)
        roll = eye_angle  # å¤´éƒ¨å€¾æ–œ
        
        return pitch, yaw, roll

    def detect_gesture(self, pitch, yaw, roll):
        """
        æ£€æµ‹ç‚¹å¤´/æ‘‡å¤´åŠ¨ä½œï¼š
        - ç‚¹å¤´ï¼špitchä¸Šä¸‹ä¸€ä¸ªæ¥å›ä¸”å¹…åº¦éƒ½è¶…è¿‡é˜ˆå€¼
        - æ‘‡å¤´ï¼šyawå·¦å³ä¸€ä¸ªæ¥å›ä¸”å¹…åº¦éƒ½è¶…è¿‡é˜ˆå€¼
        :return: 'nod'/'shake'/None
        """
        gesture = None
        # ç‚¹å¤´æ£€æµ‹
        if self.last_pitch is not None:
            delta_pitch = pitch - self.last_pitch
            # pitchæ–¹å‘å˜åŒ–çŠ¶æ€æœº
            if not hasattr(self, 'pitch_state'):
                self.pitch_state = 0  # 0:åˆå§‹, 1:å‘ä¸Š, 2:å‘ä¸‹
            if self.pitch_state == 0 and delta_pitch > self.nod_threshold:
                self.pitch_state = 1
                self.pitch_peak = pitch
            elif self.pitch_state == 1 and delta_pitch < -self.nod_threshold:
                if abs(self.pitch_peak - pitch) > self.nod_threshold * 2:
                    gesture = 'nod'
                self.pitch_state = 0
            elif self.pitch_state == 0 and delta_pitch < -self.nod_threshold:
                self.pitch_state = 2
                self.pitch_peak = pitch
            elif self.pitch_state == 2 and delta_pitch > self.nod_threshold:
                if abs(self.pitch_peak - pitch) > self.nod_threshold * 2:
                    gesture = 'nod'
                self.pitch_state = 0
        self.last_pitch = pitch
        
        # æ‘‡å¤´æ£€æµ‹
        if self.last_yaw is not None:
            delta_yaw = yaw - self.last_yaw
            if not hasattr(self, 'yaw_state'):
                self.yaw_state = 0  # 0:åˆå§‹, 1:å‘å·¦, 2:å‘å³
            if self.yaw_state == 0 and delta_yaw > self.shake_threshold:
                self.yaw_state = 1
                self.yaw_peak = yaw
            elif self.yaw_state == 1 and delta_yaw < -self.shake_threshold:
                if abs(self.yaw_peak - yaw) > self.shake_threshold * 2:
                    gesture = 'shake'
                self.yaw_state = 0
            elif self.yaw_state == 0 and delta_yaw < -self.shake_threshold:
                self.yaw_state = 2
                self.yaw_peak = yaw
            elif self.yaw_state == 2 and delta_yaw > self.shake_threshold:
                if abs(self.yaw_peak - yaw) > self.shake_threshold * 2:
                    gesture = 'shake'
                self.yaw_state = 0
        self.last_yaw = yaw
        
        return gesture

    def run(self):
        """
        ä»å¸§é˜Ÿåˆ—è¯»å–å›¾åƒï¼Œæ£€æµ‹å¤´éƒ¨å§¿æ€å¹¶å¯è§†åŒ–ç‚¹å¤´/æ‘‡å¤´åŠ¨ä½œã€‚
        """
        while True:
            if self.frame_queue is not None:
                frame = self.frame_queue.get()
                if frame is None:
                    break
            else:
                break
            
            # ä½¿ç”¨RetinaFaceè¿›è¡Œäººè„¸æ£€æµ‹
            bboxes, keypoints = self.face_detector.detect(frame)
            gesture = None
            
            if len(bboxes) > 0:
                # å–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
                keypoints_first_face = keypoints[0]
                
                # ä¼°è®¡å¤´éƒ¨å§¿åŠ¿
                pitch, yaw, roll = self.estimate_head_pose(keypoints_first_face, frame.shape)
                
                if pitch is not None and yaw is not None:
                    gesture = self.detect_gesture(pitch, yaw, roll)
                    # æ˜¾ç¤ºå§¿æ€è§’åº¦
                    cv2.putText(frame, f"Pitch: {pitch:.2f} Yaw: {yaw:.2f} Roll: {roll:.2f}", 
                               (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
                    # å¯è§†åŒ–ç‚¹å¤´/æ‘‡å¤´
                    if gesture == 'nod':
                        cv2.putText(frame, "Nod (Confirm)", (30, 60), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                    elif gesture == 'shake':
                        cv2.putText(frame, "Shake (Reject)", (30, 60), 
                                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)
                    
                # ç»˜åˆ¶äººè„¸æ¡†å’Œå…³é”®ç‚¹
                bbox = bboxes[0]
                x_min, y_min, x_max, y_max = map(int, bbox[:4])
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
                
                # ç»˜åˆ¶å…³é”®ç‚¹
                for kp in keypoints_first_face:
                    x, y = int(kp[0]), int(kp[1])
                    cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)
                    
            cv2.imshow('Head Gesture Recognition', frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break
                
        cv2.destroyAllWindows()


def run_head_gesture(frame_queue, result_queue):
    """
    å­çº¿ç¨‹ï¼šä»å¸§é˜Ÿåˆ—è¯»å–å›¾åƒï¼Œæ£€æµ‹å¤´éƒ¨å§¿åŠ¿å¹¶è¿”å›ç»“æœ
    :param frame_queue: è¾“å…¥å¸§é˜Ÿåˆ—
    :param result_queue: è¾“å‡ºç»“æœé˜Ÿåˆ—
    """
    recognizer = HeadGestureRecognizer(frame_queue=frame_queue)
    while True:
        frame = frame_queue.get()
        if frame is None:
            break
            
        # ä½¿ç”¨RetinaFaceè¿›è¡Œäººè„¸æ£€æµ‹
        bboxes, keypoints = recognizer.face_detector.detect(frame)
        gesture = None
        
        if len(bboxes) > 0:
            # å–ç¬¬ä¸€ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
            keypoints_first_face = keypoints[0]
            
            # ä¼°è®¡å¤´éƒ¨å§¿åŠ¿
            pitch, yaw, roll = recognizer.estimate_head_pose(keypoints_first_face, frame.shape)
            
            if pitch is not None and yaw is not None:
                gesture = recognizer.detect_gesture(pitch, yaw, roll)
                # ç»˜åˆ¶ç‚¹å¤´/æ‘‡å¤´è½¨è¿¹
                cx, cy = frame.shape[1]//2, frame.shape[0]//2
                if gesture == 'nod':
                    cv2.arrowedLine(frame, (cx, cy-60), (cx, cy+60), (0,0,255), 6, tipLength=0.3)
                    cv2.putText(frame, "Nod (Confirm)", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)
                elif gesture == 'shake':
                    cv2.arrowedLine(frame, (cx-60, cy), (cx+60, cy), (255,0,0), 6, tipLength=0.3)
                    cv2.putText(frame, "Shake (Reject)", (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,0,0), 2)
                cv2.putText(frame, f"Pitch: {pitch:.2f} Yaw: {yaw:.2f} Roll: {roll:.2f}", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)
                
            # ç»˜åˆ¶äººè„¸æ¡†å’Œå…³é”®ç‚¹
            bbox = bboxes[0]
            x_min, y_min, x_max, y_max = map(int, bbox[:4])
            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)
            
            # ç»˜åˆ¶å…³é”®ç‚¹
            for kp in keypoints_first_face:
                x, y = int(kp[0]), int(kp[1])
                cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)
                
        cv2.imshow('Head Gesture Recognition', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
        result_queue.put(gesture)
    cv2.destroyAllWindows()


def parse_args():
    """
    è§£æå‘½ä»¤è¡Œå‚æ•°ï¼ŒåŒ…æ‹¬æ¨¡å‹ç±»å‹ã€æƒé‡è·¯å¾„ã€è¾“å…¥æºã€è¾“å‡ºè·¯å¾„ã€æ•°æ®é›†ç­‰ã€‚
    æ ¹æ®æ•°æ®é›†è‡ªåŠ¨è¡¥å……binsã€binwidthã€angleç­‰å‚æ•°ã€‚
    """
    parser = argparse.ArgumentParser(description="Gaze estimation inference")
    parser.add_argument("--model", type=str, default="resnet18", help="Model name, default `resnet18`")
    parser.add_argument(
        "--weight",
        type=str,
        default="weights/18.pt",
        help="Path to gaze esimation model weights"
    )
    parser.add_argument("--view", action="store_true", default=True, help="Display the inference results")
    parser.add_argument("--source", type=str, default="0",
                        help="Path to source video file or camera index")
    parser.add_argument("--output", type=str, default="output.mp4", help="Path to save output file")
    parser.add_argument("--dataset", type=str, default="mpiigaze", help="Dataset name to get dataset related configs")
    parser.add_argument("--with-head-gesture", action="store_true", help="Enable head gesture recognition")
    parser.add_argument("--demo-mode", action="store_true", help="Run in multi-task demo mode only")
    args = parser.parse_args()

    # Override default values based on selected dataset
    if args.dataset in data_config:
        dataset_config = data_config[args.dataset]
        args.bins = dataset_config["bins"]
        args.binwidth = dataset_config["binwidth"]
        args.angle = dataset_config["angle"]
    else:
        raise ValueError(f"Unknown dataset: {args.dataset}. Available options: {list(data_config.keys())}")

    return args


def pre_process(image):
    """
    å¯¹è¾“å…¥çš„äººè„¸å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼š
    1. è½¬ä¸ºRGBæ ¼å¼
    2. ç¼©æ”¾åˆ°448x448
    3. è½¬ä¸ºTensorå¹¶å½’ä¸€åŒ–
    è¿”å›ï¼šshapeä¸º(1,3,448,448)çš„Tensor
    """
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.Resize(448),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    image = transform(image)
    image_batch = image.unsqueeze(0)
    return image_batch


def main(params):
    """
    æ¨ç†ä¸»å‡½æ•°ï¼š
    1. åˆå§‹åŒ–æ¨¡å‹å’Œäººè„¸æ£€æµ‹å™¨
    2. è¿›å…¥ä¸»å¾ªç¯ï¼Œé€å¸§æ£€æµ‹äººè„¸ã€ä¼°è®¡è§†çº¿ã€åˆ†å¿ƒæ£€æµ‹
    3. å®æ—¶ç»˜åˆ¶äººè„¸æ¡†ã€è§†çº¿å‘é‡ã€åˆ†å¿ƒè­¦å‘Šå’Œyawæ³¢å½¢å›¾
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    idx_tensor = torch.arange(params.bins, device=device, dtype=torch.float32)

    face_detector = uniface.RetinaFace()  # ç¬¬ä¸‰æ–¹äººè„¸æ£€æµ‹åº“

    try:
        gaze_detector = get_model(params.model, params.bins, inference_mode=True)
        state_dict = torch.load(params.weight, map_location=device)
        gaze_detector.load_state_dict(state_dict)
        logging.info("Gaze Estimation model weights loaded.")
    except Exception as e:
        logging.info(f"Exception occured while loading pre-trained weights of gaze estimation model. Exception: {e}")

    gaze_detector.to(device)    
    gaze_detector.eval()

    video_source = params.source
    if video_source.isdigit() or video_source == '0':
        cap = cv2.VideoCapture(int(video_source))
    else:
        cap = cv2.VideoCapture(video_source)
        
    if params.output:
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fourcc = cv2.VideoWriter_fourcc(*"mp4v")
        out = cv2.VideoWriter(params.output, fourcc, cap.get(cv2.CAP_PROP_FPS), (width, height))
        
    if not cap.isOpened():
        raise IOError("Cannot open webcam")
    safe_angle = 0.1  # è®¾ç½®ä¸ºå›ºå®šçš„å®‰å…¨è§’åº¦å€¼ï¼ˆ0.1å¼§åº¦ï¼Œçº¦5.7åº¦ï¼‰
    distraction_start_time = None
    distracted = False
    warning_level = 0
    last_warning_flash = 0

    # æ³¢å½¢å›¾ç›¸å…³
    angle_history = deque(maxlen=100)
    plt.ion()
    fig, ax = plt.subplots()
    ax.set_ylim(0, 0.5)  # é€‚åˆthetaèŒƒå›´
    ax.set_title('Theta Deviation (rad)')
    ax.set_xlabel('Frame')
    ax.set_ylabel('Theta (rad)')
    line, = ax.plot([], [], 'b-')

    # å¤´éƒ¨å§¿åŠ¿è¯†åˆ«ç›¸å…³å˜é‡
    head_gesture_enabled = params.with_head_gesture
    head_gesture_text = ''
    frame_queue = None
    result_queue = None
    gesture_thread = None
    
    # å¦‚æœå¯ç”¨å¤´éƒ¨å§¿åŠ¿è¯†åˆ«ï¼Œå¯åŠ¨å­çº¿ç¨‹
    if head_gesture_enabled:
        frame_queue = Queue(maxsize=5)
        result_queue = Queue(maxsize=5)
        gesture_thread = threading.Thread(target=run_head_gesture, args=(frame_queue, result_queue))
        gesture_thread.start()

    with torch.no_grad():
        while True:
            success, frame = cap.read()
            if not success:
                logging.info("Failed to obtain frame or EOF")
                break
                
            # å¦‚æœå¯ç”¨å¤´éƒ¨å§¿åŠ¿æ£€æµ‹ï¼Œå°†å¸§å‘é€ç»™å­çº¿ç¨‹
            if head_gesture_enabled and frame_queue and not frame_queue.full():
                frame_queue.put(frame.copy())
                
                # è·å–å­çº¿ç¨‹è¯†åˆ«ç»“æœ
                while not result_queue.empty():
                    gesture = result_queue.get()
                    if gesture == 'nod':
                        head_gesture_text = 'Nod (Confirm)'
                    elif gesture == 'shake':
                        head_gesture_text = 'Shake (Reject)'
                    elif gesture is None:
                        head_gesture_text = ''
                        
                # åœ¨ä¸»ç”»é¢æ˜¾ç¤ºå¤´éƒ¨å§¿æ€è¯†åˆ«ç»“æœ
                if head_gesture_text:
                    cv2.putText(frame, head_gesture_text, (30, 90), 
                                cv2.FONT_HERSHEY_SIMPLEX, 1, 
                                (0,0,255) if head_gesture_text.startswith('Nod') else (255,0,0), 2)

            bboxes, keypoints = face_detector.detect(frame)
            for bbox, keypoint in zip(bboxes, keypoints):
                x_min, y_min, x_max, y_max = map(int, bbox[:4])
                h, w = frame.shape[:2]
                x_min = max(0, x_min)
                y_min = max(0, y_min)
                x_max = min(w, x_max)
                y_max = min(h, y_max)
                if x_max <= x_min or y_max <= y_min:
                    continue  # è·³è¿‡æ— æ•ˆæ¡†
                image = frame[y_min:y_max, x_min:x_max]
                if image.size == 0:
                    continue  # è·³è¿‡ç©ºå›¾åƒ
                image = pre_process(image)
                image = image.to(device)
                pitch, yaw = gaze_detector(image)
                pitch_predicted, yaw_predicted = F.softmax(pitch, dim=1), F.softmax(yaw, dim=1)
                pitch_predicted = torch.sum(pitch_predicted * idx_tensor, dim=1) * params.binwidth - params.angle
                yaw_predicted = torch.sum(yaw_predicted * idx_tensor, dim=1) * params.binwidth - params.angle
                pitch_predicted = np.radians(pitch_predicted.cpu().numpy()[0])
                yaw_predicted = np.radians(yaw_predicted.cpu().numpy()[0])
                draw_bbox_gaze(frame, bbox, pitch_predicted, yaw_predicted)
                # å§‹ç»ˆç»˜åˆ¶äººè„¸æ£€æµ‹æ¡†
                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)                # è®¡ç®—è§†çº¿ä¸æ­£å‰æ–¹çš„3Då¤¹è§’theta
                theta = np.arccos(np.cos(pitch_predicted) * np.cos(yaw_predicted))
                # å®æ—¶æ˜¾ç¤ºthetaä¸æ­£å‰æ–¹å¤¹è§’
                cv2.putText(frame, f"Theta deviation: {theta:+.2f} rad", (x_min, y_min-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255,255,0), 2)
                # è®°å½•thetaè§’åº¦åˆ°æ³¢å½¢å†å²
                angle_history.append(theta)
                # åˆ†å¿ƒæ£€æµ‹é€»è¾‘
                if abs(theta) > safe_angle:
                    if distraction_start_time is None:
                        distraction_start_time = time.time()  # è®°å½•åˆ†å¿ƒèµ·å§‹æ—¶é—´
                    else:
                        duration = time.time() - distraction_start_time
                        if duration > 2:
                            distracted = True
                            warning_level = 1
                else:
                    distraction_start_time = None
                    distracted = False
                    warning_level = 0
                # åˆ†å¿ƒè­¦å‘Šæ˜¾ç¤º
                if distracted and warning_level > 0:
                    now = time.time()
                    flash = int(now*2)%2 == 0
                    if flash:
                        cv2.putText(frame, "WARNING: Distracted! Look Forward!", (60, 80), cv2.FONT_HERSHEY_SIMPLEX, 1.1, (0,0,255), 4)
                        cv2.rectangle(frame, (20, 20), (frame.shape[1]-20, frame.shape[0]-20), (0,0,255), 8)  # çº¢è‰²é—ªçƒè¾¹æ¡†

            if params.output:
                out.write(frame)
            if params.view:
                cv2.imshow('Demo', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            # --- æ³¢å½¢å›¾å®æ—¶åˆ·æ–° ---
            # æ›´æ–°matplotlibæ³¢å½¢çª—å£ï¼Œæ˜¾ç¤ºæœ€è¿‘100å¸§thetaè§’åº¦å˜åŒ–
            line.set_data(range(len(angle_history)), list(angle_history))
            ax.set_xlim(max(0, len(angle_history)-100), len(angle_history))
            fig.canvas.draw()
            fig.canvas.flush_events()    # æ¸…ç†èµ„æº
    cap.release()
    if params.output:
        out.release()
    cv2.destroyAllWindows()
    
    # ç»“æŸå¤´éƒ¨å§¿åŠ¿æ£€æµ‹çº¿ç¨‹
    if head_gesture_enabled and frame_queue:
        frame_queue.put(None)  # å‘é€é€€å‡ºä¿¡å·
        if gesture_thread:
            gesture_thread.join()


def run_multi_task_demo():
    """
    è¿è¡Œå¤šä»»åŠ¡æ¼”ç¤ºï¼ŒåŒ…æ‹¬å¤´éƒ¨å§¿åŠ¿è¯†åˆ«å’Œä¸»æ‘„åƒå¤´æ˜¾ç¤º
    """
    cap = cv2.VideoCapture(0)
    frame_queue = Queue(maxsize=5)
    result_queue = Queue(maxsize=5)
    gesture_thread = threading.Thread(target=run_head_gesture, args=(frame_queue, result_queue))
    gesture_thread.start()
    gesture_text = ''
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        if not frame_queue.full():
            frame_queue.put(frame.copy())
        # è·å–å­çº¿ç¨‹è¯†åˆ«ç»“æœ
        while not result_queue.empty():
            gesture = result_queue.get()
            if gesture == 'nod':
                gesture_text = 'Nod (Confirm)'
            elif gesture == 'shake':
                gesture_text = 'Shake (Reject)'
            elif gesture is None:
                gesture_text = ''
        if gesture_text:
            cv2.putText(frame, gesture_text, (30, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255) if gesture_text.startswith('Nod') else (255,0,0), 2)
        cv2.imshow('Main Camera Feed', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break
    frame_queue.put(None)
    gesture_thread.join()
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    args = parse_args()

    if args.demo_mode:
        # ä»…è¿è¡Œå¤šä»»åŠ¡æ¼”ç¤ºæ¨¡å¼
        run_multi_task_demo()
    else:
        # è¿è¡Œæ ‡å‡†æ¨¡å¼
        if not args.view and not args.output:
            raise Exception("At least one of --view or --ouput must be provided.")
        main(args)
