import sys
import os
import cv2
import numpy as np
from PyQt5.QtCore import QThread, pyqtSignal, QTimer
from PyQt5.QtGui import QImage, QPixmap

# ç¡®ä¿èƒ½æ‰¾åˆ°æ‰€éœ€æ¨¡å—
current_dir = os.path.dirname(__file__)
sys.path.append(current_dir)
sys.path.append(os.path.join(current_dir, '..', 'visual_interaction'))

# å¯¼å…¥stateæ¨¡å—
try:
    import state
except ImportError:
    print("âš ï¸  åˆ›å»ºä¸´æ—¶stateæ¨¡å—")
    class TempState:
        is_warning = False
        is_playing = False
    state = TempState()

# æ™ºèƒ½æŸ¥æ‰¾æƒé‡æ–‡ä»¶å¹¶å†³å®šä½¿ç”¨å“ªç§æ¥å£
def find_weight_file():
    """æŸ¥æ‰¾æƒé‡æ–‡ä»¶"""
    weight_paths = [
        # å½“å‰ç›®å½•ä¸‹çš„weightsæ–‡ä»¶å¤¹
        os.path.join(current_dir, 'weights', '18.pt'),
        # ä¸Šçº§ç›®å½•çš„visual_interaction/weights
        os.path.join(current_dir, '..', 'visual_interaction', 'weights', '18.pt'),
        # ç»å¯¹è·¯å¾„
        r"C:\Users\23301\Desktop\source\source\CarSoft\weights\18.pt"
    ]
    
    for path in weight_paths:
        if os.path.exists(path):
            print(f"âœ“ æ‰¾åˆ°æƒé‡æ–‡ä»¶: {path}")
            return path
    
    print("âš ï¸  æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ï¼Œæœç´¢è·¯å¾„:")
    for path in weight_paths:
        print(f"   - {path}")
    return None

# å°è¯•å¯¼å…¥è§†è§‰è¯†åˆ«æ¥å£
VISUAL_RECOGNITION_AVAILABLE = False
VisualRecognitionInterface = None
WEIGHT_FILE_PATH = find_weight_file()

try:
    if WEIGHT_FILE_PATH:
        # æƒé‡æ–‡ä»¶å­˜åœ¨ï¼Œå°è¯•å¯¼å…¥å®Œæ•´æ¥å£
        import torch
        from inference import VisualRecognitionInterface
        VISUAL_RECOGNITION_AVAILABLE = True
        print("âœ“ å®Œæ•´è§†è§‰è¯†åˆ«æ¥å£å¯ç”¨")
    else:
        # æƒé‡æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç®€åŒ–æ¥å£
        raise ImportError("Weight file not found")
        
except ImportError as e:
    print(f"âš ï¸  å®Œæ•´æ¥å£ä¸å¯ç”¨: {e}")
    print("âœ“ ä½¿ç”¨ç®€åŒ–è§†è§‰è¯†åˆ«æ¥å£...")
    
    # åˆ›å»ºç®€åŒ–çš„è§†è§‰è¯†åˆ«æ¥å£
    class SimpleVisualInterface:
        def __init__(self, model_name="opencv", weight_path=None, dataset="basic"):
            self.model_name = model_name
            self.weight_path = weight_path
            self.dataset = dataset
            self.distraction_callback = None
            self.gesture_callback = None
            
            # åˆå§‹åŒ–OpenCVæ£€æµ‹å™¨
            try:
                self.face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
                self.eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')
                print("âœ“ OpenCVäººè„¸/çœ¼éƒ¨æ£€æµ‹å™¨åˆå§‹åŒ–æˆåŠŸ")
                self.detection_available = True
            except Exception as e:
                print(f"âš ï¸  OpenCVæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.detection_available = False
            
            # åˆ†å¿ƒæ£€æµ‹ç›¸å…³å˜é‡
            self.face_history = []
            self.no_face_count = 0
            self.distraction_count = 0
            
            print("âœ“ ç®€åŒ–è§†è§‰è¯†åˆ«æ¥å£åˆå§‹åŒ–å®Œæˆ")
        
        def set_distraction_callback(self, callback):
            """è®¾ç½®åˆ†å¿ƒæ£€æµ‹å›è°ƒ"""
            self.distraction_callback = callback
            print("âœ“ åˆ†å¿ƒæ£€æµ‹å›è°ƒå·²è®¾ç½®")
        
        def set_gesture_callback(self, callback):
            """è®¾ç½®æ‰‹åŠ¿è¯†åˆ«å›è°ƒ"""
            self.gesture_callback = callback
            print("âœ“ æ‰‹åŠ¿è¯†åˆ«å›è°ƒå·²è®¾ç½®")
        
        def process_frame(self, frame):
            """å¤„ç†è§†é¢‘å¸§"""
            result = {
                "status": "opencv_mode",
                "face_detected": False,
                "distracted": False,
                "warning_level": 0,
                "confidence": 0.0,
                "face_count": 0,
                "gaze_info": "OpenCVåŸºç¡€æ¨¡å¼",
                "timestamp": cv2.getTickCount()
            }
            
            if not self.detection_available or frame is None:
                return result
            
            try:
                # è½¬æ¢ä¸ºç°åº¦å›¾
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                
                # äººè„¸æ£€æµ‹
                faces = self.face_cascade.detectMultiScale(
                    gray, 
                    scaleFactor=1.1, 
                    minNeighbors=5,
                    minSize=(50, 50)
                )
                
                if len(faces) > 0:
                    result["face_detected"] = True
                    result["face_count"] = len(faces)
                    result["confidence"] = 0.8
                    
                    # é‡ç½®æ— è„¸è®¡æ•°
                    self.no_face_count = 0
                    
                    # å–æœ€å¤§çš„äººè„¸è¿›è¡Œåˆ†æ
                    largest_face = max(faces, key=lambda face: face[2] * face[3])
                    (x, y, w, h) = largest_face
                    
                    # è®¡ç®—äººè„¸ä¸­å¿ƒä½ç½®
                    face_center_x = x + w // 2
                    face_center_y = y + h // 2
                    frame_center_x = frame.shape[1] // 2
                    frame_center_y = frame.shape[0] // 2
                    
                    # è®¡ç®—åç§»ç¨‹åº¦
                    deviation_x = abs(face_center_x - frame_center_x) / frame_center_x
                    deviation_y = abs(face_center_y - frame_center_y) / frame_center_y
                    
                    # åˆ†å¿ƒæ£€æµ‹é€»è¾‘
                    is_distracted = deviation_x > 0.25 or deviation_y > 0.2
                    
                    if is_distracted:
                        self.distraction_count += 1
                    else:
                        self.distraction_count = max(0, self.distraction_count - 1)
                    
                    # æ ¹æ®è¿ç»­åˆ†å¿ƒå¸§æ•°ç¡®å®šè­¦å‘Šçº§åˆ«
                    if self.distraction_count > 15:
                        result["distracted"] = True
                        result["warning_level"] = 3
                    elif self.distraction_count > 10:
                        result["distracted"] = True
                        result["warning_level"] = 2
                    elif self.distraction_count > 5:
                        result["distracted"] = True
                        result["warning_level"] = 1
                    
                    # è°ƒç”¨åˆ†å¿ƒæ£€æµ‹å›è°ƒ
                    if self.distraction_callback:
                        self.distraction_callback(result["distracted"], result["warning_level"])
                    
                    result["gaze_info"] = f"äººè„¸ä½ç½®: ({face_center_x}, {face_center_y}), åç§»: {deviation_x:.2f}"
                
                else:
                    # æ²¡æœ‰æ£€æµ‹åˆ°äººè„¸
                    self.no_face_count += 1
                    
                    if self.no_face_count > 30:
                        result["distracted"] = True
                        result["warning_level"] = 2
                        if self.distraction_callback:
                            self.distraction_callback(True, 2)
                        result["gaze_info"] = "æœªæ£€æµ‹åˆ°äººè„¸ - å¯èƒ½ç¦»å¼€åº§ä½"
                    else:
                        result["gaze_info"] = f"æœªæ£€æµ‹åˆ°äººè„¸ ({self.no_face_count}/30)"
                
            except Exception as e:
                print(f"ç®€åŒ–å¤„ç†é”™è¯¯: {e}")
                result["gaze_info"] = f"å¤„ç†é”™è¯¯: {str(e)}"
            
            return result
    
    VisualRecognitionInterface = SimpleVisualInterface
    VISUAL_RECOGNITION_AVAILABLE = True
    print("âœ“ ç®€åŒ–è§†è§‰è¯†åˆ«æ¥å£åˆ›å»ºæˆåŠŸ")


class FaceThread(QThread):
    # å®šä¹‰ä¿¡å·
    frame_ready = pyqtSignal(np.ndarray)
    distraction_detected = pyqtSignal(bool, int)
    gesture_detected = pyqtSignal(str)
    gaze_data = pyqtSignal(dict)
    error_occurred = pyqtSignal(str)
    
    def __init__(self, parent=None, camera_index=0):
        super().__init__(parent)
        self.camera_index = camera_index
        self.cap = None
        self.running = False
        self.visual_interface = None
        self.frame_count = 0
        self.camera_available = False
        self.visual_recognition_available = VISUAL_RECOGNITION_AVAILABLE

    def init_camera(self):
        """åˆå§‹åŒ–æ‘„åƒå¤´"""
        try:
            print("ğŸ” æ£€æµ‹å¯ç”¨æ‘„åƒå¤´...")
            # å°è¯•å¤šä¸ªæ‘„åƒå¤´ç´¢å¼•
            for index in [0, 1, 2]:
                print(f"   å°è¯•æ‘„åƒå¤´ç´¢å¼• {index}...")
                self.cap = cv2.VideoCapture(index)
                if self.cap.isOpened():
                    # æµ‹è¯•è¯»å–ä¸€å¸§
                    ret, frame = self.cap.read()
                    if ret and frame is not None:
                        self.camera_index = index
                        self.camera_available = True
                        print(f"âœ“ æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸ (ç´¢å¼•: {index})")
                        print(f"   åˆ†è¾¨ç‡: {frame.shape[1]}x{frame.shape[0]}")
                        
                        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
                        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
                        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
                        self.cap.set(cv2.CAP_PROP_FPS, 30)
                        return True
                    else:
                        print(f"   æ‘„åƒå¤´ {index} æ— æ³•è¯»å–å¸§")
                        self.cap.release()
                else:
                    print(f"   æ‘„åƒå¤´ {index} æ— æ³•æ‰“å¼€")
                    if self.cap:
                        self.cap.release()
            
            # å¦‚æœæ‰€æœ‰æ‘„åƒå¤´éƒ½æ— æ³•ä½¿ç”¨
            print("âœ— æœªæ‰¾åˆ°å¯ç”¨çš„æ‘„åƒå¤´")
            self.error_occurred.emit("No available camera found")
            return False
            
        except Exception as e:
            print(f"âœ— æ‘„åƒå¤´åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.error_occurred.emit(f"Camera initialization failed: {str(e)}")
            return False

    def init_visual_recognition(self):
        """åˆå§‹åŒ–è§†è§‰è¯†åˆ«æ¨¡å—"""
        if not VISUAL_RECOGNITION_AVAILABLE:
            print("âš ï¸  è§†è§‰è¯†åˆ«æ¨¡å—ä¸å¯ç”¨ï¼Œä»…å¯ç”¨åŸºç¡€æ‘„åƒå¤´åŠŸèƒ½")
            return True
            
        try:
            print("ğŸ§  åˆå§‹åŒ–è§†è§‰è¯†åˆ«æ¨¡å—...")
            
            if WEIGHT_FILE_PATH:
                # ä½¿ç”¨æ‰¾åˆ°çš„æƒé‡æ–‡ä»¶
                self.visual_interface = VisualRecognitionInterface(
                    model_name="resnet18",
                    weight_path=WEIGHT_FILE_PATH,
                    dataset="mpiigaze"
                )
                print("âœ“ ä½¿ç”¨å®Œæ•´çš„PyTorchæ¨¡å‹")
            else:
                # ä½¿ç”¨ç®€åŒ–æ¥å£
                self.visual_interface = VisualRecognitionInterface(
                    model_name="opencv",
                    weight_path=None,
                    dataset="basic"
                )
                print("âœ“ ä½¿ç”¨OpenCVç®€åŒ–æ¨¡å‹")
            
            # è®¾ç½®å›è°ƒå‡½æ•°
            if hasattr(self.visual_interface, 'set_distraction_callback'):
                self.visual_interface.set_distraction_callback(self.on_distraction_detected)
            if hasattr(self.visual_interface, 'set_gesture_callback'):
                self.visual_interface.set_gesture_callback(self.on_gesture_detected)
            
            print("âœ“ è§†è§‰è¯†åˆ«ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            return True
            
        except Exception as e:
            print(f"âš ï¸  è§†è§‰è¯†åˆ«åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            self.visual_interface = None
            return True

    def on_distraction_detected(self, distracted, warning_level):
        """åˆ†å¿ƒæ£€æµ‹å›è°ƒ"""
        state.is_warning = distracted
        self.distraction_detected.emit(distracted, warning_level)
        
    def on_gesture_detected(self, gesture):
        """æ‰‹åŠ¿æ£€æµ‹å›è°ƒ"""
        self.gesture_detected.emit(gesture)

    def start_recognition(self):
        """å¼€å§‹è¯†åˆ«"""
        print("ğŸš€ å¯åŠ¨è½¦è½½æ™ºèƒ½äº¤äº’ç³»ç»Ÿ...")
        
        camera_ok = self.init_camera()
        visual_ok = self.init_visual_recognition()
        
        if camera_ok:
            self.running = True
            self.start()
            if WEIGHT_FILE_PATH:
                print("âœ“ å®Œæ•´ç³»ç»Ÿå¯åŠ¨æˆåŠŸï¼ˆä½¿ç”¨PyTorchæ¨¡å‹ï¼‰")
            else:
                print("âœ“ åŸºç¡€ç³»ç»Ÿå¯åŠ¨æˆåŠŸï¼ˆä½¿ç”¨OpenCVæ¨¡å‹ï¼‰")
        else:
            print("âœ— ç³»ç»Ÿå¯åŠ¨å¤±è´¥ï¼šæ‘„åƒå¤´ä¸å¯ç”¨")
            self.error_occurred.emit("Camera not available")

    def stop_recognition(self):
        """åœæ­¢è¯†åˆ«"""
        print("ğŸ›‘ åœæ­¢è§†è§‰è¯†åˆ«ç³»ç»Ÿ...")
        self.running = False

        # é‡Šæ”¾æ‘„åƒå¤´èµ„æº
        if self.cap:
            self.cap.release()
            self.cap = None
    
        # åœæ­¢çº¿ç¨‹
        self.quit()
        self.wait(3000)  # ç­‰å¾…æœ€å¤š3ç§’
        if self.isRunning():
            self.terminate()  # å¼ºåˆ¶ç»ˆæ­¢
            self.wait(1000)
    
        print("âœ“ è§†è§‰è¯†åˆ«ç³»ç»Ÿå·²å®Œå…¨åœæ­¢")

    def run(self):
        """ä¸»è¿è¡Œå¾ªç¯"""
        if not self.cap:
            print("âœ— æ‘„åƒå¤´æœªåˆå§‹åŒ–")
            return
            
        print("â–¶ï¸  å¼€å§‹è§†é¢‘å¤„ç†å¾ªç¯...")
        while self.running:
            ret, frame = self.cap.read()
            if not ret:
                continue
                
            try:
                # å‘é€å¸§æ•°æ®
                self.frame_ready.emit(frame)
                
                # å¤„ç†è§†è§‰è¯†åˆ«
                if self.visual_interface:
                    result = self.visual_interface.process_frame(frame)
                    self.gaze_data.emit(result)
                else:
                    # åŸºç¡€æ¨¡å¼
                    self.gaze_data.emit({
                        'status': 'camera_only',
                        'message': 'çº¯æ‘„åƒå¤´æ¨¡å¼',
                        'frame_count': self.frame_count
                    })
                
                self.frame_count += 1
                self.msleep(33)  # ~30 FPS
                
            except Exception as e:
                print(f"âš ï¸  å¸§å¤„ç†é”™è¯¯: {e}")
                continue
                
        print("ğŸ”š è§†é¢‘å¤„ç†å¾ªç¯ç»“æŸ")
        if self.cap:
            self.cap.release()